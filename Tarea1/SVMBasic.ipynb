{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-y8Kil2snGk"
   },
   "source": [
    "# Code Assigment 1\n",
    "\n",
    "For this assignment you will use the following SVM implementation for classifying these datasets:\n",
    "https://archive.ics.uci.edu/ml/datasets/banknote+authentication\n",
    "\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+\n",
    "\n",
    "You should:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Specify which Machine Learning problem are you solving.\n",
    "\n",
    "**Solución:**\n",
    "\n",
    "- (Dataset 1: Banknote authentication): En el primer dataset se da un problema de clasificación en el cual se quiere dadas unas características verificar si un billete es falso o no.\n",
    "\n",
    "- (Dataset 2: Occupancy detection): En el segundo dataset se da un problema de clasificación en donde se verifica dadas unas características si una sala de una oficina esta vacía o ocupada por personas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Provide a short summary of the features and the labels you are working on.\n",
    "\n",
    "**Solución:**\n",
    "\n",
    "- (Dataset 1: Banknote authentication): Para el primer dataset se utiliza la Transformada de Wavelet (ondículas) para hallar las características relevantes:\n",
    "\n",
    "1. Varianza de la imagen Wavelet transformada: Se utiliza la transformada de ondículas para ver como es la distribución de los valores de los pixeles en cada imagen utilizando la varianza.\n",
    "2. Asimetría de la imagen Wavelet transformada: Se utiliza la transformada de ondículas para dar un valor cuantitativo de la asimetría de la imagen. En caso de que esto de 0 la imagen va a tener un comportamiento simétrico.\n",
    "3. Curtosis de la imagen Wavelet transformada: Se utiliza la transformada de ondículas para determinar la curtosis que se relaciona con la forma de la distribución de la intensidad de los píxeles en una imagen. Una imagen con una alta curtosis tendrá una distribución de intensidad con valores concentrados alrededor de la media, lo que puede indicar la presencia de bordes o detalles finos en la imagen. Por otro lado, una imagen con una baja curtosis tendrá una distribución de intensidad más uniforme, lo que puede indicar la presencia de áreas homogéneas o suaves en la imagen.\n",
    "4. Entropía de la imagen: Se determina la Entropía la cual se basa en la distribución de los valores de intensidad de los píxeles en una imagen y cuantifica la cantidad de información necesaria para describir la imagen.\n",
    "5. Clase: Esta característica va a diferenciar en caso de que el billete sea autentico (1) o falso (0). En este caso también va a actuar como la etiqueta del problema.\n",
    "\n",
    "- (Dataset 2: Occupancy detection): Se utilizaron diferentes modelos de aprendizaje estadístico para hallar las características relevantes:\n",
    "\n",
    "1. Fecha y hora: La fecha y hora son utilizados y pueden determinar si la oficina esta en horas de trabajo y diferenciar entre dias de descanso.\n",
    "2. Temperatura: Se puede utilizar la temperatura para verificar si hay personas en la oficina , porque la presencia de personas puede provocar un aumento de la temperatura debido a su calor corporal.\n",
    "3. Humedad relativa: Podemos utilizar la humedad relativa para determinar tambien si hay personas en la oficina, cuando las personas están presentes en una habitación, liberan humedad a través de la respiración, la transpiración y otras actividades. Esto puede provocar un aumento de la humedad relativa de la habitación.\n",
    "4. Luz: La luz también se puede utilizar como factor para la detección de ocupación en una habitación o edificio. Cuando las personas ingresan a una habitación, generalmente encienden las luces, lo que provoca un aumento en los niveles de esto, mismo caso cuando se apagan las luces de la oficina\n",
    "5. CO2: El dióxido de carbono (CO2) también se puede utilizar como indicador para la detección de ocupación en la oficina. Normalmente cuando exhalamos liberamos CO2. Por lo tanto, medir el nivel de CO2 en una habitación puede proporcionar una indicación en caso de que halla alguna persona presente en la oficina.\n",
    "6. Radio de humedad: La relación de humedad, es otro factor que se puede utilizar para la detección de ocupación de la oficina. Cuando las personas respiran, liberan humedad en el aire, lo que puede aumentar el nivel de humedad. \n",
    "7. Estado de ocupación: Esta característica va a diferenciar en caso de que la sala este ocupada (1) o vacía (0). En este caso también va a actuar como la etiqueta del problema.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Please answer the following questions: a) Are these datasets linearly separable? b) Are these datasets randomly chosen and c) The sample size is enough to guarantee generalization.\n",
    "\n",
    "**Solución:**\n",
    "\n",
    "**a) Are these datasets linearly separable?** Podemos determinar que los datasets son linealmente separables mediante programación lineal, si define una función objectivo sujeta a las restricciones que satisface una separación lineal. Veamos si ambos datasets pueden ser determinados por una separación lineal:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 1: Banknote authentication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset 1: Banknote authentication\n",
    "\n",
    "# Importamos librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "  \n",
    "#leemos txt data banknote\n",
    "bank = pd.read_csv(\"data_banknote_authentication.txt\", sep=\",\", header=None, names=[\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\", \"Class\"])\n",
    "#df.drop(columns=[\"Class\"], inplace=True)\n",
    "\n",
    "#Reemplazamos el 0 por -1 para hacer la clasificación\n",
    "bank.replace({'Class': 0,}, -1, inplace=True)\n",
    "\n",
    "#Características\n",
    "X_bank = bank[[\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\"]]\n",
    "#Etiquetas\n",
    "Y_bank = bank[\"Class\"]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     con: array([], dtype=float64)\n",
      "     fun: 17.65894922672102\n",
      " message: 'The algorithm terminated successfully and determined that the problem is infeasible.'\n",
      "     nit: 9\n",
      "   slack: array([-24.13099659, -12.19746079,  -3.81096303, ...,  73.05573985,\n",
      "        41.83949854,  -9.57368948])\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 2.13660264,  2.50390418,  0.11395069, 12.51040302,  0.3940887 ])\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linprog\n",
    "\n",
    "ardf = bank.to_numpy()\n",
    "\n",
    "b_ub = np.repeat(-1, ardf.shape[0]).reshape(-1,1)\n",
    "\n",
    "c_obj = np.repeat(1, bank.shape[1])\n",
    "res = linprog(c=c_obj, A_ub=ardf, b_ub=b_ub, options={\"disp\": False})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notemos que como no se pudo encontrar una solución al problema de programación lineal sujeto a las restricciones de una separación lineal entonces, los datos no son linealmente separables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 2: Occupancy detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset 2: Occupancy detection\n",
    "\n",
    "# Importamos librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "  \n",
    "#leemos txt data de ocuppancy\n",
    "Occp = pd.read_csv(\"datatest.txt\", sep=\",\", header=None, names=[\"col\",\"date\",\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\"HumidityRatio\",\"Occupancy\"])[1:]\n",
    "#df.drop(columns=[\"Class\"], inplace=True)\n",
    "\n",
    "#Pasamos a floats por la naturaleza de la data que esta en forma object\n",
    "Occp = Occp.drop([\"col\"],axis=1)\n",
    "Occp['date'] = Occp['date'].apply(lambda x: pd.to_datetime(x).to_julian_date())\n",
    "Occp[\"Light\"] = Occp[\"Light\"].astype(float)\n",
    "Occp[\"Temperature\"] = Occp[\"Temperature\"].astype(float)\n",
    "Occp[\"Humidity\"] = Occp[\"Humidity\"].astype(float)\n",
    "Occp[\"CO2\"] = Occp[\"CO2\"].astype(float)\n",
    "Occp[\"HumidityRatio\"] = Occp[\"HumidityRatio\"].astype(float)\n",
    "Occp[\"Occupancy\"] = Occp[\"Occupancy\"].astype(float)\n",
    "\n",
    "#Reemplazamos el 0 por -1 para hacer la clasificación\n",
    "Occp.replace({'Occupancy': 0,}, -1, inplace=True)\n",
    "#Características\n",
    "X_Occp = Occp[[\"date\",\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\"HumidityRatio\"]]\n",
    "#Etiquetas\n",
    "Y_Occp = Occp[\"Occupancy\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     con: array([], dtype=float64)\n",
      "     fun: 27.40646762708792\n",
      " message: 'The algorithm terminated successfully and determined that the problem is infeasible.'\n",
      "     nit: 11\n",
      "   slack: array([-24.91765367, -24.91745499, -24.91708317, ..., -24.96240283,\n",
      "       -24.96189873, -24.96030673])\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([1.16897412e-06, 1.50971079e-03, 1.26935660e-03, 1.17833805e-04,\n",
      "       4.43589034e-05, 6.56068138e+00, 2.08428438e+01])\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linprog\n",
    "\n",
    "#ar0 = df0.to_numpy()\n",
    "#ar1 = df1.to_numpy()\n",
    "ardf = Occp.to_numpy()\n",
    "\n",
    "b_ub = np.repeat(-1, ardf.shape[0]).reshape(-1,1)\n",
    "\n",
    "c_obj = np.repeat(1, Occp.shape[1])\n",
    "res = linprog(c=c_obj, A_ub=ardf, b_ub=b_ub, options={\"disp\": False})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notemos que como no se pudo encontrar una solución al problema de programación lineal sujeto a las restricciones de una separación lineal entonces, los datos no son linealmente separables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**b) Are these datasets randomly chosen?** En este caso con ambos datasets, puede ser difícil determinar con certeza si un conjunto es realmente aleatorio, ya que en estos casos un conjunto aleatorio no debe tener patrones ni sesgos perceptibles, pero incluso los procesos aparentemente aleatorios pueden exhibir patrones o sesgos.\n",
    "\n",
    "**c) is the sample size is enough to guarantee generalization?** Una regla general que se uso en ambos casos es tener un mínimo de 30 muestras para cada variable predictora en un modelo de regresión. Para ambos datasets, la cantidad de datos sobrepasan los miles, y tambien la cantidad de características evaluadas es pequeña por lo tanto es posible considerar que efectivamente es suficiente para la generalización. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Provide an explanation how and why the code is working. You can add comments and/or formal explanations into the notebook.\n",
    "\n",
    "**Solución:**\n",
    "\n",
    "El código suministrado no estaba funcionando ya que este solo podía trabajar con 2 características, por lo que entonces se realizo una implementación de SVM nueva:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SVM:\n",
    "\n",
    "    def __init__(self, C = 1.0):\n",
    "        # Constructor para inicializar el modelo SVM con un parámetro de penalización C y pesos w y sesgo b\n",
    "        self.C = C\n",
    "        self.w = 0\n",
    "        self.b = 0\n",
    "\n",
    "    def hingeloss(self, w, b, x, y):\n",
    "        #  Calcula la pérdida de bisagra\n",
    "        reg = 0.5 * (w * w) # Término de regularización\n",
    "\n",
    "        for i in range(x.shape[0]):\n",
    "            opt_term = y[i] * ((np.dot(w, x[i])) + b) # Término de optimización\n",
    "            loss = reg + self.C * max(0, 1-opt_term) # Calcula la pérdida usando la funcion de pérdida de bisagra\n",
    "        return loss[0][0]\n",
    "\n",
    "    def fit(self, X, Y, batch_size=100, learning_rate=0.001, epochs=1000):\n",
    "        # Entrena el SVM usando decenso de gradiente\n",
    "\n",
    "        number_of_features = X.shape[1] # Número de características en X\n",
    "        number_of_samples = X.shape[0] # Número de datos en X\n",
    "        c = self.C \n",
    "\n",
    "        ids = np.arange(number_of_samples)\n",
    "\n",
    "        # Randomiza las muestras\n",
    "        np.random.shuffle(ids)\n",
    "\n",
    "        w = np.zeros((1, number_of_features)) # Inicializa los pesos a 0\n",
    "        b = 0 # bias es 0\n",
    "        losses = [] # guarda una pérdida en cada iteración\n",
    "\n",
    "        for i in range(epochs):\n",
    "            # Calcula la pérdida\n",
    "            l = self.hingeloss(w, b, X, Y)\n",
    "            losses.append(l) \n",
    "\n",
    "            for batch_initial in range(0, number_of_samples, batch_size):\n",
    "                gradw = 0\n",
    "                gradb = 0\n",
    "\n",
    "                for j in range(batch_initial, batch_initial+ batch_size):\n",
    "                    if j < number_of_samples:\n",
    "                        x = ids[j]\n",
    "                        ti = Y[x] * (np.dot(w, X[x].T) + b)\n",
    "\n",
    "                        if ti > 1:\n",
    "                            gradw += 0 # gradiente es 0 si ti > 1\n",
    "                            gradb += 0\n",
    "                        else:\n",
    "                            # Calcula los gradientes\n",
    "                            gradw += c * Y[x] * X[x] \n",
    "                            gradb += c * Y[x] \n",
    "\n",
    "                w = w - learning_rate * w + learning_rate * gradw\n",
    "                b = b + learning_rate * gradb\n",
    "        \n",
    "        self.w = w # Actualiza los pesos aprendidos\n",
    "        self.b = b # Actualiza los bias aprendidos\n",
    "\n",
    "        return self.w, self.b, losses # Retorna los pesos, bias y pérdidas\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predice usando el modelo SVM\n",
    "        \n",
    "        prediction = np.dot(X, self.w[0]) + self.b # w.x + b\n",
    "        return np.sign(prediction) # Retorna las etiquetas predecidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso funciona ya que nos va a encontrar un límite de desición el cual va a clasificar los datos de acuerdo a 2 etiquetas, y con esto podremos diferenciar si un billete es falso o no (Banknote authentication), o si hay alguna persona en la oficina o no (Occupancy detection).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Show some examples to illustrate that the method is working properly.\n",
    "\n",
    "**Solución:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 1: Banknote authentication**\n",
    "\n",
    "Separemos la data en training y test y utilicemos el modelo SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9883381924198251"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BankSVM = SVM()\n",
    "\n",
    "#Hay que dividir la data en train y test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bank, Y_bank, test_size=0.25, random_state=104, shuffle=True)\n",
    "\n",
    "#Ahora entrenemos la SVM\n",
    "BankSVM.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "\n",
    "#Veamos que tan bueno es el rendimiento\n",
    "Predict = BankSVM.predict(X_test.to_numpy())\n",
    "\n",
    "res = (Predict - y_test)\n",
    "#Resultados que fallan\n",
    "inc = np.count_nonzero(res)\n",
    "\n",
    "#Score\n",
    "Score = 1 - inc/len(Predict)\n",
    "print(Score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos que en este caso el modelo tiene un muy buen rendimiento con la data de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 2: Occupancy detection**\n",
    "\n",
    "La data ya esta separada en este caso pero esta en otros archivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset 2: Occupancy detection\n",
    "\n",
    "# Importamos librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Como los datos están separados de una vez para\n",
    "# entrenar y hacer testing del modelo, es necesario \n",
    "# crear varios dataframes\n",
    "\n",
    "def occupancy_df_process(path):\n",
    "  data = pd.read_csv(path, sep=\",\", \n",
    "            names=[\"col\",\"date\",\"Temperature\",\"Humidity\",\n",
    "                   \"Light\",\"CO2\",\"HumidityRatio\",\"Occupancy\"])[1:]\n",
    "\n",
    "  data = data.drop([\"col\"],axis=1)\n",
    "  data['date'] = data['date'].apply(lambda x: pd.to_datetime(x).to_julian_date())\n",
    "  data[\"Light\"] = data[\"Light\"].astype(float)\n",
    "  data[\"Temperature\"] = data[\"Temperature\"].astype(float)\n",
    "  data[\"Humidity\"] = data[\"Humidity\"].astype(float)\n",
    "  data[\"CO2\"] = data[\"CO2\"].astype(float)\n",
    "  data[\"HumidityRatio\"] = data[\"HumidityRatio\"].astype(float)\n",
    "  data[\"Occupancy\"] = data[\"Occupancy\"].astype(float)\n",
    "\n",
    "\n",
    "  # Remplazamos los 0 por -1 para la SVM\n",
    "  data[\"Occupancy\"] = data[\"Occupancy\"].replace([0],-1)\n",
    "\n",
    "  # Etiquetas\n",
    "  X_data = data[[\"date\",\"Temperature\",\"Humidity\",\n",
    "                   \"Light\",\"CO2\",\"HumidityRatio\"]]\n",
    "  # Atributos\n",
    "  Y_data = data[\"Occupancy\"]\n",
    "  return X_data, Y_data\n",
    "\n",
    "Occp_train_X , Occp_train_y = occupancy_df_process(\"datatraining.txt\")\n",
    "Occp_test_X , Occp_test_y = occupancy_df_process(\"datatest.txt\")\n",
    "Occp_test2_X , Occp_test2_y = occupancy_df_process(\"datatest2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usemos el modelo SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7898892534864643\n"
     ]
    }
   ],
   "source": [
    "OccSVM = SVM()\n",
    "\n",
    "#Hay que dividir la data en train y test\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_bank, Y_bank, test_size=0.25, random_state=104, shuffle=True)\n",
    "\n",
    "#Ahora entrenemos la SVM\n",
    "OccSVM.fit(Occp_train_X.to_numpy(), Occp_train_y.to_numpy())\n",
    "\n",
    "#Veamos que tan bueno es el rendimiento\n",
    "Predict = OccSVM.predict(Occp_test2_X.to_numpy())\n",
    "\n",
    "res = (Predict - Occp_test2_y)\n",
    "#Resultados que fallan\n",
    "inc = np.count_nonzero(res)\n",
    "\n",
    "#Score\n",
    "Score = 1 - inc/len(Predict)\n",
    "\n",
    "#Nota la fecha se convirtió a juliana y afecta el rendimiento.\n",
    "print(Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso el modelo no predice tan bien los datos de test, lo cual puede ser provocado por el atributo de fecha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Provide quantitative evidence for generalization using the provided dataset.\n",
    "\n",
    "**Solución:**\n",
    "\n",
    "Para esto vamos a usar validación cruzada (Cross-validation) que nos sirve para evaluar los resultados de un análisis estadístico y garantizar que son independientes de la partición entre datos de entrenamiento y prueba. El rendimiento promedio en todos los test se puede utilizar como una estimación del rendimiento de generalización del modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 1: Banknote authentication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99 precisión con desviacion de 0.01\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "scores = cross_val_score(clf, X_bank, Y_bank, cv=5)\n",
    "scores\n",
    "\n",
    "print(\"%0.2f precisión con desviacion de %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se puede ver que los datos van a tener un muy buen rendimiento de generalización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 2: Occupancy detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93 precisión con desviacion de 0.06\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "scores = cross_val_score(clf, X_Occp, Y_Occp, cv=5)\n",
    "scores\n",
    "\n",
    "print(\"%0.2f precisión con desviacion de %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se puede ver que los datos van a tener un buen rendimiento de generalización. La gran desviación puede estar provocada por la característica de fecha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Navegacion entre los Notebooks:**\n",
    "\n",
    "Regresar a [Introducción de la tarea 1](Introducción_Tarea_1.ipynb)<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Autores:** Alejandro Martin Salcedo.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
